In probability theory, the normal is the standard deviation is the variance.  The simplest case of a normal distribution is known as the standard normal distribution. This is a special case when and , and it is described by this probability density function: : The factor in this expression ensures that the total area under the curve is equal to one. The factor in the exponent ensures that the distribution has unit variance distributions:. NOTE: the following table shows , not as defined above. For small , the quantile function has the useful asymptotic expansion. The normal distribution is the only distribution whose cumulants beyond the first two . More generally, any linear combination of independent normal deviates is a normal deviate. For any positive integer , any normal distribution with mean and variance is the distribution of the sum of independent normal deviates, each with mean and variance . This property is called infinite divisibility. Conversely, if and are independent random variables and their sum has a normal distribution, then both and must be normal deviates. This result is known as Cramér’s decomposition theorem, and is equivalent to saying that the convolution of two distributions is normal if and only if both are normal. Cramér's theorem implies that a linear combination of independent non-Gaussian variables will never have an exactly normal distribution, although it may approach it arbitrarily closely. Bernstein's theorem states that if and are independent and and are also independent, then both X and Y must necessarily have normal distributions. More generally, if , ..., are independent random variables, then two distinct linear combinations and will be independent if and only if all are normal and , where denotes the variance of ..  The central limit theorem states that under certain are the mean, variance, and third central moment respectively. One of the main practical uses of the Gaussian law is to model the empirical distributions of many different random variables encountered in practice. In such case a possible extension would be a richer family of distributions, having more than two parameters and therefore being able to fit the empirical distribution more accurately. The examples of such extensions are: Pearson distribution — a four-parameter family of probability distributions that extend the normal law to include different skewness and kurtosis values. The generalized normal distribution, also known as the exponential power distribution, allows for distribution tails with thicker or thinner asymptotic behaviors..  It is often the case that we don't know the parameters of the normal distribution, but instead want to estimate them. That is, having a sample , with parameters that are the same as the update equations above.. The occurrence of normal distribution in practical problems can be loosely classified into four categories: # Exactly normal distributions; # Approximately normal laws, for example when such approximation is justified by the central limit theorem; and # Distributions modeled as normal – the normal distribution being the distribution with maximum entropy for a given mean and variance. # Regression problems – the normal distribution being found after systematic effects have been modeled sufficiently well. Certain quantities in physics are distributed normally, as was first demonstrated by James Clerk Maxwell. Examples of such quantities are: Probability density function of a ground state in a quantum harmonic oscillator. The position of a particle that experiences diffusion. If initially the particle is located at a specific point , normal curve equivalents, stanines, z-scores, and T-scores. Additionally, some behavioral statistical procedures assume that scores are normally distributed; for example, t-tests and ANOVAs. Bell curve grading assigns relative grades based on a normal distribution of scores. In hydrology the distribution of long duration river discharge or rainfall, e.g. monthly and yearly totals, is often thought to be practically normal according to the central limit theorem. The blue picture, made with CumFreq, illustrates an example of fitting the normal distribution to ranked October rainfalls showing the 90% confidence belt based on the binomial distribution. The rainfall data are represented by plotting positions as part of the cumulative frequency analysis. In regression analysis, lack of normality in residuals simply indicates that the model postulated is inadequate in accounting for the tendency in the data and needs to be augmented; in other words, normality in residuals can always be achieved given a properly constructed model..  In computer simulations, especially in applications of the Monte-Carlo method, it is often desirable to generate values that are normally distributed. The algorithms listed below all generate the standard normal deviates, since a can be generated as , where Z is standard normal. All these algorithms rely on the availability of a random number generator U capable of producing uniform random variates. The most straightforward method is based on the probability integral transform property: if U is distributed uniformly on . Some more approximations can be found at: . In particular, small relative error on the whole domain for the CDF and the quantile function as well, is achieved via an explicitly invertible formula by Sergei Winitzki in 2008..  Some authors attribute the credit for the discovery of the normal distribution to de Moivre, who in 1738 published in the second edition of his "The Doctrine of Chances" the study of the coefficients in the binomial expansion of . De Moivre proved that the middle term in this expansion has the approximate magnitude of , and that "If m or ½n be a Quantity infinitely great, then the Logarithm of the Ratio, which a Term distant from the middle by the Interval ℓ, has to the middle Term, is ." Although this theorem can be interpreted as the first obscure expression for the normal probability law, Stigler points out that de Moivre himself did not interpret his results as anything more than the approximate rule for the binomial coefficients, and in particular de Moivre lacked the concept of the probability density function. In 1809 Gauss published his monograph "Theoria motus corporum coelestium in sectionibus conicis solem ambientium" where among other things he introduces several important statistical concepts, such as the method of least squares, the method of maximum likelihood, and the normal distribution. Gauss used M, , to denote the measurements of some unknown quantity&nbsp;V, and sought the "most probable" estimator of that quantity: the one that maximizes the probability of obtaining the observed experimental results. In his notation φΔ is the probability law of the measurement errors of magnitude Δ. Not knowing what the function φ is, Gauss requires that his method should reduce to the well-known answer: the arithmetic mean of the measured values. Starting from these principles, Gauss demonstrates that the only law that rationalizes the choice of arithmetic mean as an estimator of the location parameter, is the normal law of errors: : where h is "the measure of the precision of the observations". Using this normal law as a generic model for errors in the experiments, Gauss formulates what is now known as the non-linear weighted least squares "Introduction to the theory of statistics". When the name is used, the "Gaussian distribution" was named after Carl Friedrich Gauss, who introduced the distribution in 1809 as a way of rationalizing the method of least squares as outlined above. Among English speakers, both "normal distribution" and "Gaussian distribution" are in common use, with different terms preferred by different communities.